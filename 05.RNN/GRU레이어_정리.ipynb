{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU\r\n",
    "LSTM과 비슷한 역할을 하지만, 더 간단한 구조로 이뤄져 있어 계산상 효율적이다. 이것은 기존의 LSTM에서 사용되는 셀 상태 계산(은닉 상태 업데이트)을 줄였다. 또한, 특정 문제에서는 LSTM보다 더 적합한 레이어다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU 구조\r\n",
    "- GRU는 LSTM에서의 셀 상태(cell state) 역할의 c가 없다. cell state의 역할을 다음의 출력 h에서 그 역할을 함께 한다.\r\n",
    "- GRU에서는 Update Gate, Reset Gate 두 가지만 존재하며 사용되는 활성화 함수는 sigmoid 2번과 tanh 1번 사용된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Reset Gate : 과거의 데이터를 리셋시키는 것을 목적으로 하는 게이트, sigmoid 연산을 통해 과거의 데이터를 얼마나 리셋시킬지에 대한 값인 r(0과 1사이)을 출력\r\n",
    "- Update Gate : 과거와 현재의 정보 업데이트 비율을 결정하는 게이트, 출력값 u는 현시점에서의 가져가야 할 데이터 양을 결정하는 값. 1-u는 잊어버려야 할 데이터 양이라고 생각하면 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 데이터 선정\r\n",
    "과거의 출력 값 데이터를 그대로 이용하지 않고, 리셋 데이터로 출력한 값을 이용하여 pointwise 곱 연산을 한다. 연산 결과는 tanh함수를 한번 거치게 된다.\r\n",
    "- 출력값 계산\r\n",
    "u는 데이터 중 얼마나 가져갈 것이지를, 1-u는 얼마나 잊을 것인지를 의미한다. tanh를 거친 h~는 현시점에서 리셋된 데이터를 의미하며 해당 데이터 중 가져갈 것을 연산한다. 이전의 h에서 얼마나 잊을지를 연산하여 이것들을 모두 합한 값이 출력값이 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. ]\n",
      "  [0.1]\n",
      "  [0.2]\n",
      "  [0.3]]\n",
      "\n",
      " [[0.1]\n",
      "  [0.2]\n",
      "  [0.3]\n",
      "  [0.4]]\n",
      "\n",
      " [[0.2]\n",
      "  [0.3]\n",
      "  [0.4]\n",
      "  [0.5]]\n",
      "\n",
      " [[0.3]\n",
      "  [0.4]\n",
      "  [0.5]\n",
      "  [0.6]]\n",
      "\n",
      " [[0.4]\n",
      "  [0.5]\n",
      "  [0.6]\n",
      "  [0.7]]\n",
      "\n",
      " [[0.5]\n",
      "  [0.6]\n",
      "  [0.7]\n",
      "  [0.8]]]\n",
      "[0.4 0.5 0.6 0.7 0.8 0.9]\n"
     ]
    }
   ],
   "source": [
    "# test 환경\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "X = []\r\n",
    "Y = []\r\n",
    "for i in range(6):\r\n",
    "    lst = list(range(i,i+4))\r\n",
    "    X.append(list(map(lambda c: [c/10], lst)))\r\n",
    "    Y.append((i+4)/10)\r\n",
    "X = np.array(X)\r\n",
    "Y = np.array(Y)\r\n",
    "print(X)\r\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 만들기 & 학습\r\n",
    "from tensorflow.keras.models import Sequential\r\n",
    "from tensorflow.keras.layers import Embedding, GRU, Dense\r\n",
    "\r\n",
    "model = Sequential([GRU(units=32,\r\n",
    "              # units 파라미터는 RNN 신경망에 존재하는 뉴런의 개수\r\n",
    "                    return_sequences=False,\r\n",
    "        # RNN 계산 과정에 있는 hidden state를 출력할 것인지에 대한 값을 의미\r\n",
    "        # 해당 값은 다층으로 이루어진 RNN 또는 one-to-many, many-to-many 출력을 위해서 사용\r\n",
    "        # False의 경우, 마지막 출력값 하나를 출력/ True의 경우, 모든 과정의 출력 값을 출력\r\n",
    "                    input_shape=[4,1]),\r\n",
    "                    Dense(1, activation='tanh')])\r\n",
    "# 모델은 Sequential에 simpleRNN과 Dense레이어를 하나씩 추가\r\n",
    "# input_shape는 4 time-step마다 하나의 답이라서 [4,1] 선언\r\n",
    "# 학습을 반복하면서 mse 값을 낮추는 훈련 함\r\n",
    "\r\n",
    "model.compile(optimizer='adam',\r\n",
    "              loss='mse',\r\n",
    "              metrics=['accuracy'])\r\n",
    "\r\n",
    "history = model.fit(X, Y, epochs=200, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.43649778]\n",
      " [0.5346203 ]\n",
      " [0.6234701 ]\n",
      " [0.70119697]\n",
      " [0.7669906 ]\n",
      " [0.82101744]]\n"
     ]
    }
   ],
   "source": [
    "# 기존의 훈련 데이터 세트를 넣어서 예측\r\n",
    "print(model.predict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.9237259]]\n"
     ]
    }
   ],
   "source": [
    "# 1.2에 많이 가깝진 않지만 훈련데이터를 증가시켜서 해당 결과를 개선시킬 수 있음\r\n",
    "# RNN 사용시 0.912, LSTM 사용시 0.936, GRU 사용시 0.923\r\n",
    "X_test = np.array([[[0.8],[0.9],[1.0],[1.1]]])\r\n",
    "print(model.predict(X_test))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8c1eae21719a0790335dcb83aad72b63b602cfe5cdb2bda0f60bc11d4f154e4b"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
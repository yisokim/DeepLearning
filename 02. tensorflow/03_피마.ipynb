{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 피마인디언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\r\n",
    "import numpy \r\n",
    "import tensorflow as tf\r\n",
    "from tensorflow import keras\r\n",
    "from tensorflow.keras.models import Sequential\r\n",
    "#층을 담는 그릇(Sequential은 keras에 포함되어 있는데\r\n",
    "# 딥러닝 모델을 한층 한층 쌓기 쉽게 해주는 함수)\r\n",
    "from tensorflow.keras.layers import Dense\r\n",
    "#층을 만드는 것(각 층이 가질 특성을 각각 다르게 지정 가능)  \r\n",
    "import warnings\r\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed값 생성\r\n",
    "seed = 2021\r\n",
    "np.random.seed(seed)\r\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0    1   2   3    4     5      6   7  8\n",
       "0  6  148  72  35    0  33.6  0.627  50  1\n",
       "1  1   85  66  29    0  26.6  0.351  31  0\n",
       "2  8  183  64   0    0  23.3  0.672  32  1\n",
       "3  1   89  66  23   94  28.1  0.167  21  0\n",
       "4  0  137  40  35  168  43.1  2.288  33  1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 로드\r\n",
    "df = pd.read_csv('dataset/pima-indians-diabetes.csv',header=None)\r\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((576, 8), (192, 8), (576,), (192,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\r\n",
    "X_train, X_test, y_train, y_test = train_test_split(\r\n",
    "    df.iloc[:,:-1].values, df.iloc[:,-1].values,\r\n",
    "    stratify=df.iloc[:,-1].values, random_state=seed\r\n",
    ")\r\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_2 (Dense)              (None, 16)                144       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 289\n",
      "Trainable params: 289\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 모델 설정 \r\n",
    "model = Sequential()   # 2, 4,, 8, 16, 32, 64, 128, 256, 512, 1024 2진법 구조로 생각\r\n",
    "model.add(Dense(16, input_dim=8, activation = 'relu'))  #0부터 7까지였으니 8개 인풋 #히든 레이어\r\n",
    "model.add(Dense(8, activation = 'relu'))   #히든레이어 \r\n",
    "model.add(Dense(1, activation = 'sigmoid')) #이진분류 #아웃풋레이어\r\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 컴파일\r\n",
    "model.compile(\r\n",
    "    loss = 'binary_crossentropy',\r\n",
    "    optimizer ='adam',\r\n",
    "    metrics=['accuracy']\r\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.4452 - accuracy: 0.8043 - val_loss: 0.6115 - val_accuracy: 0.6552\n",
      "Epoch 2/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4420 - accuracy: 0.7935 - val_loss: 0.6929 - val_accuracy: 0.6810\n",
      "Epoch 3/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4657 - accuracy: 0.7826 - val_loss: 0.5992 - val_accuracy: 0.6897\n",
      "Epoch 4/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4587 - accuracy: 0.7848 - val_loss: 0.5985 - val_accuracy: 0.6810\n",
      "Epoch 5/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4459 - accuracy: 0.8087 - val_loss: 0.6274 - val_accuracy: 0.6810\n",
      "Epoch 6/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4448 - accuracy: 0.7717 - val_loss: 0.5929 - val_accuracy: 0.6810\n",
      "Epoch 7/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4490 - accuracy: 0.7674 - val_loss: 0.6185 - val_accuracy: 0.6724\n",
      "Epoch 8/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4326 - accuracy: 0.7913 - val_loss: 0.6420 - val_accuracy: 0.6810\n",
      "Epoch 9/200\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.4473 - accuracy: 0.7957 - val_loss: 0.5921 - val_accuracy: 0.6810\n",
      "Epoch 10/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4415 - accuracy: 0.7935 - val_loss: 0.6124 - val_accuracy: 0.6897\n",
      "Epoch 11/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4426 - accuracy: 0.7913 - val_loss: 0.6068 - val_accuracy: 0.6983\n",
      "Epoch 12/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4474 - accuracy: 0.7957 - val_loss: 0.6315 - val_accuracy: 0.6379\n",
      "Epoch 13/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4475 - accuracy: 0.7870 - val_loss: 0.6267 - val_accuracy: 0.6466\n",
      "Epoch 14/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4467 - accuracy: 0.7978 - val_loss: 0.7303 - val_accuracy: 0.6552\n",
      "Epoch 15/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4531 - accuracy: 0.7870 - val_loss: 0.5835 - val_accuracy: 0.6983\n",
      "Epoch 16/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4379 - accuracy: 0.7957 - val_loss: 0.6359 - val_accuracy: 0.6724\n",
      "Epoch 17/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4380 - accuracy: 0.7935 - val_loss: 0.6118 - val_accuracy: 0.6552\n",
      "Epoch 18/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4400 - accuracy: 0.8022 - val_loss: 0.5867 - val_accuracy: 0.6724\n",
      "Epoch 19/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4401 - accuracy: 0.8087 - val_loss: 0.5980 - val_accuracy: 0.6810\n",
      "Epoch 20/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4472 - accuracy: 0.7978 - val_loss: 0.6291 - val_accuracy: 0.6810\n",
      "Epoch 21/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4495 - accuracy: 0.7848 - val_loss: 0.6045 - val_accuracy: 0.6379\n",
      "Epoch 22/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4503 - accuracy: 0.7848 - val_loss: 0.6066 - val_accuracy: 0.6638\n",
      "Epoch 23/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4549 - accuracy: 0.7957 - val_loss: 0.6382 - val_accuracy: 0.6810\n",
      "Epoch 24/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4925 - accuracy: 0.7630 - val_loss: 0.5968 - val_accuracy: 0.7069\n",
      "Epoch 25/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4433 - accuracy: 0.7935 - val_loss: 0.6011 - val_accuracy: 0.6897\n",
      "Epoch 26/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4479 - accuracy: 0.7978 - val_loss: 0.5941 - val_accuracy: 0.6897\n",
      "Epoch 27/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4350 - accuracy: 0.8000 - val_loss: 0.6527 - val_accuracy: 0.6897\n",
      "Epoch 28/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4636 - accuracy: 0.7848 - val_loss: 0.6126 - val_accuracy: 0.6724\n",
      "Epoch 29/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4405 - accuracy: 0.7978 - val_loss: 0.6042 - val_accuracy: 0.6983\n",
      "Epoch 30/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4401 - accuracy: 0.8217 - val_loss: 0.6414 - val_accuracy: 0.6810\n",
      "Epoch 31/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4508 - accuracy: 0.8022 - val_loss: 0.5960 - val_accuracy: 0.6638\n",
      "Epoch 32/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4260 - accuracy: 0.7935 - val_loss: 0.6766 - val_accuracy: 0.6466\n",
      "Epoch 33/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4473 - accuracy: 0.7891 - val_loss: 0.6582 - val_accuracy: 0.6897\n",
      "Epoch 34/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4468 - accuracy: 0.7891 - val_loss: 0.6054 - val_accuracy: 0.6724\n",
      "Epoch 35/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4405 - accuracy: 0.7978 - val_loss: 0.6338 - val_accuracy: 0.6810\n",
      "Epoch 36/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4360 - accuracy: 0.7978 - val_loss: 0.6036 - val_accuracy: 0.6810\n",
      "Epoch 37/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4503 - accuracy: 0.7913 - val_loss: 0.5948 - val_accuracy: 0.6810\n",
      "Epoch 38/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4418 - accuracy: 0.7957 - val_loss: 0.6409 - val_accuracy: 0.6897\n",
      "Epoch 39/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4479 - accuracy: 0.7826 - val_loss: 0.5994 - val_accuracy: 0.6724\n",
      "Epoch 40/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4355 - accuracy: 0.7978 - val_loss: 0.6217 - val_accuracy: 0.6897\n",
      "Epoch 41/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4491 - accuracy: 0.7870 - val_loss: 0.6348 - val_accuracy: 0.6897\n",
      "Epoch 42/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4498 - accuracy: 0.7891 - val_loss: 0.6047 - val_accuracy: 0.6638\n",
      "Epoch 43/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4375 - accuracy: 0.7957 - val_loss: 0.6108 - val_accuracy: 0.6810\n",
      "Epoch 44/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4393 - accuracy: 0.7913 - val_loss: 0.6896 - val_accuracy: 0.6810\n",
      "Epoch 45/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4471 - accuracy: 0.7957 - val_loss: 0.6404 - val_accuracy: 0.6466\n",
      "Epoch 46/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4402 - accuracy: 0.8043 - val_loss: 0.6393 - val_accuracy: 0.6810\n",
      "Epoch 47/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4609 - accuracy: 0.7674 - val_loss: 0.6113 - val_accuracy: 0.6810\n",
      "Epoch 48/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4474 - accuracy: 0.8043 - val_loss: 0.6437 - val_accuracy: 0.6897\n",
      "Epoch 49/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4291 - accuracy: 0.7957 - val_loss: 0.6391 - val_accuracy: 0.6810\n",
      "Epoch 50/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4475 - accuracy: 0.8022 - val_loss: 0.6063 - val_accuracy: 0.6810\n",
      "Epoch 51/200\n",
      "46/46 [==============================] - 0s 1ms/step - loss: 0.4657 - accuracy: 0.7783 - val_loss: 0.6226 - val_accuracy: 0.6897\n",
      "Epoch 52/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4425 - accuracy: 0.7870 - val_loss: 0.6198 - val_accuracy: 0.6897\n",
      "Epoch 53/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4508 - accuracy: 0.7913 - val_loss: 0.5942 - val_accuracy: 0.6724\n",
      "Epoch 54/200\n",
      "46/46 [==============================] - 0s 1ms/step - loss: 0.4329 - accuracy: 0.8022 - val_loss: 0.6098 - val_accuracy: 0.6810\n",
      "Epoch 55/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4478 - accuracy: 0.8022 - val_loss: 0.6072 - val_accuracy: 0.6897\n",
      "Epoch 56/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4424 - accuracy: 0.7891 - val_loss: 0.6401 - val_accuracy: 0.6724\n",
      "Epoch 57/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4575 - accuracy: 0.7804 - val_loss: 0.6284 - val_accuracy: 0.6897\n",
      "Epoch 58/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4417 - accuracy: 0.7978 - val_loss: 0.6064 - val_accuracy: 0.6638\n",
      "Epoch 59/200\n",
      "46/46 [==============================] - 0s 1ms/step - loss: 0.4437 - accuracy: 0.7848 - val_loss: 0.6264 - val_accuracy: 0.6638\n",
      "Epoch 60/200\n",
      "46/46 [==============================] - 0s 1ms/step - loss: 0.4329 - accuracy: 0.8065 - val_loss: 0.6119 - val_accuracy: 0.6724\n",
      "Epoch 61/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4385 - accuracy: 0.8065 - val_loss: 0.6040 - val_accuracy: 0.6897\n",
      "Epoch 62/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4445 - accuracy: 0.7913 - val_loss: 0.6268 - val_accuracy: 0.6466\n",
      "Epoch 63/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4423 - accuracy: 0.8043 - val_loss: 0.6418 - val_accuracy: 0.6897\n",
      "Epoch 64/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4324 - accuracy: 0.8087 - val_loss: 0.6410 - val_accuracy: 0.6552\n",
      "Epoch 65/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4489 - accuracy: 0.7891 - val_loss: 0.5946 - val_accuracy: 0.6810\n",
      "Epoch 66/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4383 - accuracy: 0.7891 - val_loss: 0.5933 - val_accuracy: 0.6897\n",
      "Epoch 67/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4299 - accuracy: 0.8130 - val_loss: 0.6005 - val_accuracy: 0.6810\n",
      "Epoch 68/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4436 - accuracy: 0.7957 - val_loss: 0.5948 - val_accuracy: 0.6897\n",
      "Epoch 69/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4317 - accuracy: 0.8022 - val_loss: 0.5932 - val_accuracy: 0.6810\n",
      "Epoch 70/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4443 - accuracy: 0.8043 - val_loss: 0.6048 - val_accuracy: 0.6897\n",
      "Epoch 71/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4342 - accuracy: 0.8022 - val_loss: 0.6045 - val_accuracy: 0.6724\n",
      "Epoch 72/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4437 - accuracy: 0.7957 - val_loss: 0.6124 - val_accuracy: 0.6897\n",
      "Epoch 73/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4373 - accuracy: 0.8109 - val_loss: 0.6177 - val_accuracy: 0.6638\n",
      "Epoch 74/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4414 - accuracy: 0.7957 - val_loss: 0.6643 - val_accuracy: 0.6897\n",
      "Epoch 75/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4476 - accuracy: 0.8022 - val_loss: 0.6513 - val_accuracy: 0.6724\n",
      "Epoch 76/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4295 - accuracy: 0.7978 - val_loss: 0.6225 - val_accuracy: 0.6897\n",
      "Epoch 77/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4382 - accuracy: 0.8000 - val_loss: 0.6462 - val_accuracy: 0.6810\n",
      "Epoch 78/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4291 - accuracy: 0.8043 - val_loss: 0.6350 - val_accuracy: 0.6724\n",
      "Epoch 79/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4404 - accuracy: 0.7935 - val_loss: 0.6096 - val_accuracy: 0.6724\n",
      "Epoch 80/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4377 - accuracy: 0.7978 - val_loss: 0.6938 - val_accuracy: 0.6724\n",
      "Epoch 81/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4445 - accuracy: 0.7935 - val_loss: 0.6297 - val_accuracy: 0.6897\n",
      "Epoch 82/200\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.4333 - accuracy: 0.8152 - val_loss: 0.6177 - val_accuracy: 0.6897\n",
      "Epoch 83/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4445 - accuracy: 0.8065 - val_loss: 0.6160 - val_accuracy: 0.6293\n",
      "Epoch 84/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4334 - accuracy: 0.8000 - val_loss: 0.5912 - val_accuracy: 0.6897\n",
      "Epoch 85/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4459 - accuracy: 0.7957 - val_loss: 0.6081 - val_accuracy: 0.6810\n",
      "Epoch 86/200\n",
      "46/46 [==============================] - 0s 1ms/step - loss: 0.4411 - accuracy: 0.8130 - val_loss: 0.6075 - val_accuracy: 0.6724\n",
      "Epoch 87/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4338 - accuracy: 0.8043 - val_loss: 0.6079 - val_accuracy: 0.6897\n",
      "Epoch 88/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4416 - accuracy: 0.7913 - val_loss: 0.6214 - val_accuracy: 0.6466\n",
      "Epoch 89/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4364 - accuracy: 0.7957 - val_loss: 0.6269 - val_accuracy: 0.6638\n",
      "Epoch 90/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4295 - accuracy: 0.8087 - val_loss: 0.6224 - val_accuracy: 0.6897\n",
      "Epoch 91/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4442 - accuracy: 0.7891 - val_loss: 0.6078 - val_accuracy: 0.6638\n",
      "Epoch 92/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4388 - accuracy: 0.7804 - val_loss: 0.6074 - val_accuracy: 0.6897\n",
      "Epoch 93/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4376 - accuracy: 0.8043 - val_loss: 0.6107 - val_accuracy: 0.6897\n",
      "Epoch 94/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4353 - accuracy: 0.7978 - val_loss: 0.6467 - val_accuracy: 0.6810\n",
      "Epoch 95/200\n",
      "46/46 [==============================] - 0s 1ms/step - loss: 0.4425 - accuracy: 0.8043 - val_loss: 0.6238 - val_accuracy: 0.6552\n",
      "Epoch 96/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4357 - accuracy: 0.8022 - val_loss: 0.6137 - val_accuracy: 0.6724\n",
      "Epoch 97/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4269 - accuracy: 0.8130 - val_loss: 0.6190 - val_accuracy: 0.6897\n",
      "Epoch 98/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4378 - accuracy: 0.7957 - val_loss: 0.6247 - val_accuracy: 0.6983\n",
      "Epoch 99/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4372 - accuracy: 0.7935 - val_loss: 0.6261 - val_accuracy: 0.6810\n",
      "Epoch 100/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4345 - accuracy: 0.8000 - val_loss: 0.6376 - val_accuracy: 0.6638\n",
      "Epoch 101/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4423 - accuracy: 0.8152 - val_loss: 0.6415 - val_accuracy: 0.6552\n",
      "Epoch 102/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4430 - accuracy: 0.8065 - val_loss: 0.6691 - val_accuracy: 0.6983\n",
      "Epoch 103/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4283 - accuracy: 0.8065 - val_loss: 0.6461 - val_accuracy: 0.6724\n",
      "Epoch 104/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4315 - accuracy: 0.7935 - val_loss: 0.6146 - val_accuracy: 0.6724\n",
      "Epoch 105/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4278 - accuracy: 0.8043 - val_loss: 0.6129 - val_accuracy: 0.6897\n",
      "Epoch 106/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4370 - accuracy: 0.7957 - val_loss: 0.6317 - val_accuracy: 0.6466\n",
      "Epoch 107/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4314 - accuracy: 0.7848 - val_loss: 0.6281 - val_accuracy: 0.6552\n",
      "Epoch 108/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4362 - accuracy: 0.8022 - val_loss: 0.6170 - val_accuracy: 0.6552\n",
      "Epoch 109/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4546 - accuracy: 0.7804 - val_loss: 0.6016 - val_accuracy: 0.6724\n",
      "Epoch 110/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4294 - accuracy: 0.8109 - val_loss: 0.6007 - val_accuracy: 0.6897\n",
      "Epoch 111/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4269 - accuracy: 0.7913 - val_loss: 0.6009 - val_accuracy: 0.6897\n",
      "Epoch 112/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4361 - accuracy: 0.7957 - val_loss: 0.5906 - val_accuracy: 0.6983\n",
      "Epoch 113/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4288 - accuracy: 0.8000 - val_loss: 0.6409 - val_accuracy: 0.6638\n",
      "Epoch 114/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4339 - accuracy: 0.8065 - val_loss: 0.6145 - val_accuracy: 0.6897\n",
      "Epoch 115/200\n",
      "46/46 [==============================] - 0s 1ms/step - loss: 0.4369 - accuracy: 0.8043 - val_loss: 0.6037 - val_accuracy: 0.6552\n",
      "Epoch 116/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4331 - accuracy: 0.8022 - val_loss: 0.6107 - val_accuracy: 0.6638\n",
      "Epoch 117/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4380 - accuracy: 0.8000 - val_loss: 0.6314 - val_accuracy: 0.6466\n",
      "Epoch 118/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4351 - accuracy: 0.7891 - val_loss: 0.6137 - val_accuracy: 0.6897\n",
      "Epoch 119/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4284 - accuracy: 0.7826 - val_loss: 0.6308 - val_accuracy: 0.6983\n",
      "Epoch 120/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4361 - accuracy: 0.8109 - val_loss: 0.6257 - val_accuracy: 0.6638\n",
      "Epoch 121/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4325 - accuracy: 0.8065 - val_loss: 0.6103 - val_accuracy: 0.6897\n",
      "Epoch 122/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4257 - accuracy: 0.8130 - val_loss: 0.6096 - val_accuracy: 0.6810\n",
      "Epoch 123/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4305 - accuracy: 0.8109 - val_loss: 0.6217 - val_accuracy: 0.6897\n",
      "Epoch 124/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4404 - accuracy: 0.7891 - val_loss: 0.6241 - val_accuracy: 0.6724\n",
      "Epoch 125/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4566 - accuracy: 0.7935 - val_loss: 0.6537 - val_accuracy: 0.6379\n",
      "Epoch 126/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4387 - accuracy: 0.8022 - val_loss: 0.6187 - val_accuracy: 0.6810\n",
      "Epoch 127/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4293 - accuracy: 0.7957 - val_loss: 0.6152 - val_accuracy: 0.6897\n",
      "Epoch 128/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4357 - accuracy: 0.8087 - val_loss: 0.6415 - val_accuracy: 0.6724\n",
      "Epoch 129/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4280 - accuracy: 0.7913 - val_loss: 0.6417 - val_accuracy: 0.6897\n",
      "Epoch 130/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4287 - accuracy: 0.8196 - val_loss: 0.6295 - val_accuracy: 0.6552\n",
      "Epoch 131/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4307 - accuracy: 0.7978 - val_loss: 0.6337 - val_accuracy: 0.6897\n",
      "Epoch 132/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4238 - accuracy: 0.8065 - val_loss: 0.6243 - val_accuracy: 0.6810\n",
      "Epoch 133/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4385 - accuracy: 0.8087 - val_loss: 0.6170 - val_accuracy: 0.6810\n",
      "Epoch 134/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4310 - accuracy: 0.8152 - val_loss: 0.6497 - val_accuracy: 0.6724\n",
      "Epoch 135/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4279 - accuracy: 0.8043 - val_loss: 0.6402 - val_accuracy: 0.6552\n",
      "Epoch 136/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4378 - accuracy: 0.8043 - val_loss: 0.6648 - val_accuracy: 0.6810\n",
      "Epoch 137/200\n",
      "46/46 [==============================] - 0s 1ms/step - loss: 0.4434 - accuracy: 0.7913 - val_loss: 0.6298 - val_accuracy: 0.6810\n",
      "Epoch 138/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4244 - accuracy: 0.8000 - val_loss: 0.6322 - val_accuracy: 0.6724\n",
      "Epoch 139/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4328 - accuracy: 0.7913 - val_loss: 0.6180 - val_accuracy: 0.6810\n",
      "Epoch 140/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4506 - accuracy: 0.7957 - val_loss: 0.6289 - val_accuracy: 0.6638\n",
      "Epoch 141/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4362 - accuracy: 0.8000 - val_loss: 0.5940 - val_accuracy: 0.6810\n",
      "Epoch 142/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4253 - accuracy: 0.8022 - val_loss: 0.6390 - val_accuracy: 0.6724\n",
      "Epoch 143/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4301 - accuracy: 0.8087 - val_loss: 0.6279 - val_accuracy: 0.6552\n",
      "Epoch 144/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4300 - accuracy: 0.8065 - val_loss: 0.5966 - val_accuracy: 0.6810\n",
      "Epoch 145/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4351 - accuracy: 0.8065 - val_loss: 0.6000 - val_accuracy: 0.7069\n",
      "Epoch 146/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4290 - accuracy: 0.8043 - val_loss: 0.5970 - val_accuracy: 0.6897\n",
      "Epoch 147/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4318 - accuracy: 0.8065 - val_loss: 0.6388 - val_accuracy: 0.6724\n",
      "Epoch 148/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4396 - accuracy: 0.7804 - val_loss: 0.6367 - val_accuracy: 0.6810\n",
      "Epoch 149/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4259 - accuracy: 0.8022 - val_loss: 0.6463 - val_accuracy: 0.6379\n",
      "Epoch 150/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4294 - accuracy: 0.8174 - val_loss: 0.6814 - val_accuracy: 0.6552\n",
      "Epoch 151/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4468 - accuracy: 0.8000 - val_loss: 0.6228 - val_accuracy: 0.6552\n",
      "Epoch 152/200\n",
      "46/46 [==============================] - 0s 1ms/step - loss: 0.4237 - accuracy: 0.8239 - val_loss: 0.6041 - val_accuracy: 0.7069\n",
      "Epoch 153/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4381 - accuracy: 0.7891 - val_loss: 0.6379 - val_accuracy: 0.6724\n",
      "Epoch 154/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4326 - accuracy: 0.8043 - val_loss: 0.6109 - val_accuracy: 0.6897\n",
      "Epoch 155/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4331 - accuracy: 0.8174 - val_loss: 0.6095 - val_accuracy: 0.6983\n",
      "Epoch 156/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4253 - accuracy: 0.8022 - val_loss: 0.6158 - val_accuracy: 0.6724\n",
      "Epoch 157/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4312 - accuracy: 0.8087 - val_loss: 0.6133 - val_accuracy: 0.6897\n",
      "Epoch 158/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4399 - accuracy: 0.7891 - val_loss: 0.6092 - val_accuracy: 0.6983\n",
      "Epoch 159/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4312 - accuracy: 0.8000 - val_loss: 0.6175 - val_accuracy: 0.6810\n",
      "Epoch 160/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4393 - accuracy: 0.8022 - val_loss: 0.6281 - val_accuracy: 0.6810\n",
      "Epoch 161/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4345 - accuracy: 0.8000 - val_loss: 0.6302 - val_accuracy: 0.6638\n",
      "Epoch 162/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4253 - accuracy: 0.8065 - val_loss: 0.6201 - val_accuracy: 0.6724\n",
      "Epoch 163/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4276 - accuracy: 0.8087 - val_loss: 0.6488 - val_accuracy: 0.6810\n",
      "Epoch 164/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4158 - accuracy: 0.8087 - val_loss: 0.6587 - val_accuracy: 0.6724\n",
      "Epoch 165/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4229 - accuracy: 0.8130 - val_loss: 0.6337 - val_accuracy: 0.6724\n",
      "Epoch 166/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4309 - accuracy: 0.8000 - val_loss: 0.7018 - val_accuracy: 0.6897\n",
      "Epoch 167/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4379 - accuracy: 0.7935 - val_loss: 0.6402 - val_accuracy: 0.6810\n",
      "Epoch 168/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4258 - accuracy: 0.8065 - val_loss: 0.6300 - val_accuracy: 0.6983\n",
      "Epoch 169/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4272 - accuracy: 0.8043 - val_loss: 0.6613 - val_accuracy: 0.6724\n",
      "Epoch 170/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4253 - accuracy: 0.8043 - val_loss: 0.6273 - val_accuracy: 0.6810\n",
      "Epoch 171/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4185 - accuracy: 0.8043 - val_loss: 0.6397 - val_accuracy: 0.6552\n",
      "Epoch 172/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4557 - accuracy: 0.7913 - val_loss: 0.6599 - val_accuracy: 0.6638\n",
      "Epoch 173/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4354 - accuracy: 0.7957 - val_loss: 0.6540 - val_accuracy: 0.6724\n",
      "Epoch 174/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4354 - accuracy: 0.8043 - val_loss: 0.6372 - val_accuracy: 0.6724\n",
      "Epoch 175/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4204 - accuracy: 0.8087 - val_loss: 0.6290 - val_accuracy: 0.6983\n",
      "Epoch 176/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4226 - accuracy: 0.8109 - val_loss: 0.6695 - val_accuracy: 0.6724\n",
      "Epoch 177/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4292 - accuracy: 0.8000 - val_loss: 0.6401 - val_accuracy: 0.6897\n",
      "Epoch 178/200\n",
      "46/46 [==============================] - 0s 1ms/step - loss: 0.4228 - accuracy: 0.8109 - val_loss: 0.6755 - val_accuracy: 0.6724\n",
      "Epoch 179/200\n",
      "46/46 [==============================] - 0s 3ms/step - loss: 0.4281 - accuracy: 0.8065 - val_loss: 0.6477 - val_accuracy: 0.6810\n",
      "Epoch 180/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4276 - accuracy: 0.8109 - val_loss: 0.6494 - val_accuracy: 0.6379\n",
      "Epoch 181/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4332 - accuracy: 0.7978 - val_loss: 0.6710 - val_accuracy: 0.6724\n",
      "Epoch 182/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4276 - accuracy: 0.8065 - val_loss: 0.6628 - val_accuracy: 0.6724\n",
      "Epoch 183/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4270 - accuracy: 0.8043 - val_loss: 0.6861 - val_accuracy: 0.6897\n",
      "Epoch 184/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4283 - accuracy: 0.8000 - val_loss: 0.6606 - val_accuracy: 0.6897\n",
      "Epoch 185/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4252 - accuracy: 0.7978 - val_loss: 0.6363 - val_accuracy: 0.7069\n",
      "Epoch 186/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4252 - accuracy: 0.8022 - val_loss: 0.6840 - val_accuracy: 0.6638\n",
      "Epoch 187/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4469 - accuracy: 0.7913 - val_loss: 0.6526 - val_accuracy: 0.6897\n",
      "Epoch 188/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4214 - accuracy: 0.8152 - val_loss: 0.6537 - val_accuracy: 0.6810\n",
      "Epoch 189/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4216 - accuracy: 0.8087 - val_loss: 0.6455 - val_accuracy: 0.6983\n",
      "Epoch 190/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4149 - accuracy: 0.8109 - val_loss: 0.6790 - val_accuracy: 0.6552\n",
      "Epoch 191/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4280 - accuracy: 0.8065 - val_loss: 0.6425 - val_accuracy: 0.6897\n",
      "Epoch 192/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4207 - accuracy: 0.8043 - val_loss: 0.6445 - val_accuracy: 0.6897\n",
      "Epoch 193/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4312 - accuracy: 0.7957 - val_loss: 0.6636 - val_accuracy: 0.6379\n",
      "Epoch 194/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4208 - accuracy: 0.8109 - val_loss: 0.6980 - val_accuracy: 0.6724\n",
      "Epoch 195/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4252 - accuracy: 0.7957 - val_loss: 0.6778 - val_accuracy: 0.6552\n",
      "Epoch 196/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4255 - accuracy: 0.8065 - val_loss: 0.7112 - val_accuracy: 0.6897\n",
      "Epoch 197/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4402 - accuracy: 0.7870 - val_loss: 0.6855 - val_accuracy: 0.6810\n",
      "Epoch 198/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4337 - accuracy: 0.8065 - val_loss: 0.6970 - val_accuracy: 0.7155\n",
      "Epoch 199/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4263 - accuracy: 0.8109 - val_loss: 0.6577 - val_accuracy: 0.6810\n",
      "Epoch 200/200\n",
      "46/46 [==============================] - 0s 2ms/step - loss: 0.4279 - accuracy: 0.8065 - val_loss: 0.6572 - val_accuracy: 0.6897\n"
     ]
    }
   ],
   "source": [
    "# 모델 실행\r\n",
    "history = model.fit(X_train, y_train, validation_split = 0.2, epochs=200, batch_size = 10) # verbose=0 화면에 표시 안됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 998us/step - loss: 0.5076 - accuracy: 0.7812\n",
      "Accuracy: 0.7812\n"
     ]
    }
   ],
   "source": [
    "# 결과 출력  ( 정확도 )\r\n",
    "print( 'Accuracy: %.4f' % (model.evaluate(X_test,y_test)[1]))   # 4f : 소수점 넷째 자리 까지"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 30/1, 100 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "모델 실행\r\n",
    "history = model.fit(\r\n",
    "    X_train, y_train,\r\n",
    "    validation_split = 0.2,\r\n",
    "    epochs=100,\r\n",
    "    batch_size = 10,\r\n",
    "    verbose=0)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8c1eae21719a0790335dcb83aad72b63b602cfe5cdb2bda0f60bc11d4f154e4b"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}